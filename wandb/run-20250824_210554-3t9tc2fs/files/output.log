***** Running training *****
08/24/2025 21:05:55 - INFO - utils.data_utils -   ***** Running training *****
  Num examples = 11995
08/24/2025 21:05:55 - INFO - utils.data_utils -     Num examples = 11995
  Dataloader size = 1999
08/24/2025 21:05:55 - INFO - utils.data_utils -     Dataloader size = 1999
  Resume training from step 0
08/24/2025 21:05:55 - INFO - utils.data_utils -     Resume training from step 0
  Instantaneous batch size per device = 2
08/24/2025 21:05:55 - INFO - utils.data_utils -     Instantaneous batch size per device = 2
  Total train batch size (w. data & sequence parallel, accumulation) = 144.0
08/24/2025 21:05:55 - INFO - utils.data_utils -     Total train batch size (w. data & sequence parallel, accumulation) = 144.0
  Gradient Accumulation steps = 12
08/24/2025 21:05:55 - INFO - utils.data_utils -     Gradient Accumulation steps = 12
  Total optimization steps per epoch = 300
08/24/2025 21:05:55 - INFO - utils.data_utils -     Total optimization steps per epoch = 300
  Total training parameters per FSDP shard = 11.90140832 B
08/24/2025 21:05:55 - INFO - utils.data_utils -     Total training parameters per FSDP shard = 11.90140832 B
  Master weight dtype: torch.bfloat16
08/24/2025 21:05:55 - INFO - utils.data_utils -     Master weight dtype: torch.bfloat16
Sampling Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.54s/it]
08/24/2025 21:06:27 - INFO - utils.data_utils -   --> decode image and save to log dir...███████████████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.52s/it]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.52s/it]
08/24/2025 21:06:58 - INFO - utils.data_utils -   --> decode image and save to log dir...███████████████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.53s/it]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.52s/it]
08/24/2025 21:07:29 - INFO - utils.data_utils -   --> decode image and save to log dir...███████████████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.53s/it]
--> decode image and save to log dir...
Sampling Progress:  50%|████████████████████████████████████████████████████████████████                                                                | 10/20 [00:15<00:15,  1.56s/it]
Traceback (most recent call last):██████████████████████████████████████████████████████                                                                | 10/20 [00:15<00:15,  1.53s/it]
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1429, in <module>
    main(args)
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1074, in main
    loss, grad_norm = train_one_step(
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 705, in train_one_step
    rewards, all_latents, all_log_probs, sigma_schedule, all_image_ids, all_condition_latents, all_condition_ids, all_condition_type_ids = sample_reference_model(
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 533, in sample_reference_model
    z, latents, batch_latents, batch_log_probs, batch_condition_latents, batch_condition_ids, batch_condition_type_ids = run_sample_step(
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 346, in run_sample_step
    pred = tranformer_forward(
  File "/root/autodl-tmp/DiffusionGRPO/src/flux/transformer_.py", line 161, in tranformer_forward
    encoder_hidden_states, hidden_states, condition_latents = block_forward(
  File "/root/autodl-tmp/DiffusionGRPO/src/flux/block.py", line 188, in block_forward
    norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(
  File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1429, in <module>
[rank0]:     main(args)
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1074, in main
[rank0]:     loss, grad_norm = train_one_step(
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 705, in train_one_step
[rank0]:     rewards, all_latents, all_log_probs, sigma_schedule, all_image_ids, all_condition_latents, all_condition_ids, all_condition_type_ids = sample_reference_model(
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 533, in sample_reference_model
[rank0]:     z, latents, batch_latents, batch_log_probs, batch_condition_latents, batch_condition_ids, batch_condition_type_ids = run_sample_step(
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 346, in run_sample_step
[rank0]:     pred = tranformer_forward(
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/flux/transformer_.py", line 161, in tranformer_forward
[rank0]:     encoder_hidden_states, hidden_states, condition_latents = block_forward(
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/flux/block.py", line 188, in block_forward
[rank0]:     norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(
[rank0]:   File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]: KeyboardInterrupt
