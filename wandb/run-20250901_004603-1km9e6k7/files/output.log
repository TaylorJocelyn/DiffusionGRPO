***** Running training *****
09/01/2025 00:46:04 - INFO - utils.data_utils -   ***** Running training *****
  Num examples = 11995
09/01/2025 00:46:04 - INFO - utils.data_utils -     Num examples = 11995
  Dataloader size = 1500
09/01/2025 00:46:04 - INFO - utils.data_utils -     Dataloader size = 1500
  Resume training from step 0
09/01/2025 00:46:04 - INFO - utils.data_utils -     Resume training from step 0
  Instantaneous batch size per device = 1
09/01/2025 00:46:04 - INFO - utils.data_utils -     Instantaneous batch size per device = 1
  Total train batch size (w. data & sequence parallel, accumulation) = 32.0
09/01/2025 00:46:04 - INFO - utils.data_utils -     Total train batch size (w. data & sequence parallel, accumulation) = 32.0
  Gradient Accumulation steps = 4
09/01/2025 00:46:04 - INFO - utils.data_utils -     Gradient Accumulation steps = 4
  Total optimization steps per epoch = 300
09/01/2025 00:46:04 - INFO - utils.data_utils -     Total optimization steps per epoch = 300
  Total training parameters per FSDP shard = 11.90140832 B
09/01/2025 00:46:04 - INFO - utils.data_utils -     Total training parameters per FSDP shard = 11.90140832 B
  Master weight dtype: torch.bfloat16
09/01/2025 00:46:04 - INFO - utils.data_utils -     Master weight dtype: torch.bfloat16
Sampling Progress: 100%|████████████████████████████████████████████████████████████████| 16/16 [00:13<00:00,  1.22it/s]
09/01/2025 00:46:18 - INFO - utils.data_utils -   --> decode image and save to log dir... 16/16 [00:13<00:00,  1.90it/s]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████| 16/16 [00:08<00:00,  1.94it/s]
09/01/2025 00:46:27 - INFO - utils.data_utils -   --> decode image and save to log dir... 16/16 [00:08<00:00,  1.92it/s]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████| 16/16 [00:08<00:00,  1.94it/s]
09/01/2025 00:46:36 - INFO - utils.data_utils -   --> decode image and save to log dir... 16/16 [00:08<00:00,  1.92it/s]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████| 16/16 [00:08<00:00,  1.94it/s]
09/01/2025 00:46:45 - INFO - utils.data_utils -   --> decode image and save to log dir... 16/16 [00:08<00:00,  1.92it/s]
--> decode image and save to log dir...
gathered_reward {'ctr_reward': [], 'ecp_reward': [], 't2i_reward': [], 'hps_reward': tensor([0.0938, 0.0939, 0.0939, 0.0939, 0.0543, 0.0544, 0.0542, 0.0544, 0.0473,
        0.0474, 0.0474, 0.0474, 0.0543, 0.0544, 0.0544, 0.0544, 0.0623, 0.0623,
        0.0623, 0.0623, 0.0427, 0.0427, 0.0427, 0.0428, 0.0579, 0.0579, 0.0579,
        0.0579, 0.0625, 0.0624, 0.0623, 0.0625], device='cuda:0')}
idx i:  0
ratio: 1.00
09/01/2025 00:47:29 - INFO - utils.data_utils -   ratio: 1.00
advantage -1.4998
09/01/2025 00:47:29 - INFO - utils.data_utils -   advantage -1.4998
final loss: 0.0417
09/01/2025 00:47:29 - INFO - utils.data_utils -   final loss: 0.0417
idx i:  1
ratio: 1.00
09/01/2025 00:48:18 - INFO - utils.data_utils -   ratio: 1.00
advantage 0.4999
09/01/2025 00:48:18 - INFO - utils.data_utils -   advantage 0.4999
final loss: -0.0139
09/01/2025 00:48:18 - INFO - utils.data_utils -   final loss: -0.0139
idx i:  2
ratio: 1.00
09/01/2025 00:49:04 - INFO - utils.data_utils -   ratio: 1.00
advantage 0.4999
09/01/2025 00:49:04 - INFO - utils.data_utils -   advantage 0.4999
final loss: -0.0139
09/01/2025 00:49:04 - INFO - utils.data_utils -   final loss: -0.0139
idx i:  3
grad_norm_
Traceback (most recent call last):
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1535, in <module>
    main(args)
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1181, in main
    loss, grad_norm = train_one_step(
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 940, in train_one_step
    grad_norm = transformer.clip_grad_norm_(max_grad_norm)
  File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/diffusers/models/modeling_utils.py", line 173, in __getattr__
    return super().__getattr__(name)
  File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
    raise AttributeError(
AttributeError: 'FSDPFluxTransformer2DModel' object has no attribute 'clip_grad_norm_'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1535, in <module>
[rank0]:     main(args)
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1181, in main
[rank0]:     loss, grad_norm = train_one_step(
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 940, in train_one_step
[rank0]:     grad_norm = transformer.clip_grad_norm_(max_grad_norm)
[rank0]:   File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/diffusers/models/modeling_utils.py", line 173, in __getattr__
[rank0]:     return super().__getattr__(name)
[rank0]:   File "/root/miniconda3/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1928, in __getattr__
[rank0]:     raise AttributeError(
[rank0]: AttributeError: 'FSDPFluxTransformer2DModel' object has no attribute 'clip_grad_norm_'
