***** Running training *****
08/25/2025 00:34:00 - INFO - utils.data_utils -   ***** Running training *****
  Num examples = 11995
08/25/2025 00:34:00 - INFO - utils.data_utils -     Num examples = 11995
  Dataloader size = 1500
08/25/2025 00:34:00 - INFO - utils.data_utils -     Dataloader size = 1500
  Resume training from step 0
08/25/2025 00:34:00 - INFO - utils.data_utils -     Resume training from step 0
  Instantaneous batch size per device = 1
08/25/2025 00:34:00 - INFO - utils.data_utils -     Instantaneous batch size per device = 1
  Total train batch size (w. data & sequence parallel, accumulation) = 32.0
08/25/2025 00:34:00 - INFO - utils.data_utils -     Total train batch size (w. data & sequence parallel, accumulation) = 32.0
  Gradient Accumulation steps = 4
08/25/2025 00:34:00 - INFO - utils.data_utils -     Gradient Accumulation steps = 4
  Total optimization steps per epoch = 300
08/25/2025 00:34:00 - INFO - utils.data_utils -     Total optimization steps per epoch = 300
  Total training parameters per FSDP shard = 1.491297992 B
08/25/2025 00:34:00 - INFO - utils.data_utils -     Total training parameters per FSDP shard = 1.491297992 B
  Master weight dtype: torch.bfloat16
08/25/2025 00:34:00 - INFO - utils.data_utils -     Master weight dtype: torch.bfloat16
Sampling Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:20<00:00,  1.30s/it]
08/25/2025 00:34:21 - INFO - utils.data_utils -   --> decode image and save to log dir...███████████████████████████████████████████████████████████████| 16/16 [00:20<00:00,  1.51it/s]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.53it/s]
08/25/2025 00:34:33 - INFO - utils.data_utils -   --> decode image and save to log dir...███████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.54it/s]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.53it/s]
08/25/2025 00:34:44 - INFO - utils.data_utils -   --> decode image and save to log dir...███████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.54it/s]
--> decode image and save to log dir...
Sampling Progress: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.53it/s]
08/25/2025 00:34:55 - INFO - utils.data_utils -   --> decode image and save to log dir...███████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.54it/s]
--> decode image and save to log dir...
gathered_reward {'ctr_reward': [], 'ecp_reward': [], 't2i_reward': [], 'hps_reward': tensor([0.0682, 0.0703, 0.0709, 0.0691, 0.0233, 0.0217, 0.0273, 0.0265, 0.0626,
        0.0587, 0.0579, 0.0563, 0.0256, 0.0246, 0.0237, 0.0278, 0.0771, 0.0797,
        0.0796, 0.0760, 0.0210, 0.0191, 0.0198, 0.0181, 0.0749, 0.0777, 0.0779,
        0.0783, 0.0776, 0.0764, 0.0772, 0.0825], device='cuda:0')}
samples['latents'].shape:  torch.Size([4, 15, 2704, 64])
samples['timesteps'].shape:  torch.Size([4, 15])
samples['next_latents'].shape:  torch.Size([4, 15, 2704, 64])
samples['encoder_hidden_states'].shape:  torch.Size([4, 512, 4096])
samples['pooled_prompt_embeds'].shape:  torch.Size([4, 768])
samples['image_ids'].shape:  torch.Size([4, 2704, 3])
samples['condition_latents'].shape:  torch.Size([4, 1, 3072, 64])
samples['condition_ids'].shape:  torch.Size([4, 3072, 3])
samples['text_ids'].shape:  torch.Size([4, 3])
len samples_batched_list:  1
idx i:  0
ratio: 1.00
08/25/2025 00:35:22 - INFO - utils.data_utils -   ratio: 1.00
advantage -1.1864
08/25/2025 00:35:22 - INFO - utils.data_utils -   advantage -1.1864
final loss: 0.0330
08/25/2025 00:35:22 - INFO - utils.data_utils -   final loss: 0.0330
Traceback (most recent call last):
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1456, in <module>
    main(args)
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1102, in main
    loss, grad_norm = train_one_step(
  File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 904, in train_one_step
    return total_loss, grad_norm.item()
UnboundLocalError: local variable 'grad_norm' referenced before assignment
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1456, in <module>
[rank0]:     main(args)
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 1102, in main
[rank0]:     loss, grad_norm = train_one_step(
[rank0]:   File "/root/autodl-tmp/DiffusionGRPO/src/train_grpo_flux.py", line 904, in train_one_step
[rank0]:     return total_loss, grad_norm.item()
[rank0]: UnboundLocalError: local variable 'grad_norm' referenced before assignment
