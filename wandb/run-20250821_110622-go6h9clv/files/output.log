***** Running training *****
08/21/2025 11:06:27 - INFO - utils.data_utils -   ***** Running training *****
  Num examples = 5
08/21/2025 11:06:27 - INFO - utils.data_utils -     Num examples = 5
  Dataloader size = 5
08/21/2025 11:06:27 - INFO - utils.data_utils -     Dataloader size = 5
  Resume training from step 0
08/21/2025 11:06:27 - INFO - utils.data_utils -     Resume training from step 0
  Instantaneous batch size per device = 1
08/21/2025 11:06:27 - INFO - utils.data_utils -     Instantaneous batch size per device = 1
  Total train batch size (w. data & sequence parallel, accumulation) = 4.0
08/21/2025 11:06:27 - INFO - utils.data_utils -     Total train batch size (w. data & sequence parallel, accumulation) = 4.0
  Gradient Accumulation steps = 4
08/21/2025 11:06:27 - INFO - utils.data_utils -     Gradient Accumulation steps = 4
  Total optimization steps per epoch = 300
08/21/2025 11:06:27 - INFO - utils.data_utils -     Total optimization steps per epoch = 300
  Total training parameters per FSDP shard = 0.028975616 B
08/21/2025 11:06:27 - INFO - utils.data_utils -     Total training parameters per FSDP shard = 0.028975616 B
  Master weight dtype: torch.bfloat16
08/21/2025 11:06:27 - INFO - utils.data_utils -     Master weight dtype: torch.bfloat16
Sampling Progress: 100%|███████████████████████████████████████████████████████████████████| 16/16 [00:23<00:00,  1.48s/it]
08/21/2025 11:06:54 - INFO - utils.data_utils -   --> decode image and save to log dir...██| 16/16 [00:23<00:00,  1.44s/it]
--> decode image and save to log dir...
Sampling Progress: 100%|███████████████████████████████████████████████████████████████████| 16/16 [00:23<00:00,  1.44s/it]
08/21/2025 11:07:18 - INFO - utils.data_utils -   --> decode image and save to log dir...██| 16/16 [00:23<00:00,  1.44s/it]
--> decode image and save to log dir...
gathered_reward {'ctr_reward': [], 'ecp_reward': [], 't2i_reward': [], 'hps_reward': tensor([0.2460, 0.2396], device='cuda:0')}
Traceback (most recent call last):
  File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py", line 39, in <module>
    cli.main()
  File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 430, in main
    run()
  File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 284, in run_file
    runpy.run_path(target, run_name="__main__")
  File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path
    return _run_module_code(code, init_globals, run_name,
  File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code
    exec(code, run_globals)
  File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 1342, in <module>
    main(args)
  File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 987, in main
    loss, grad_norm = train_one_step(
  File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 755, in train_one_step
    new_log_probs = grpo_one_step(
  File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 352, in grpo_one_step
    pred = tranformer_forward(
  File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/flux/transformer.py", line 209, in tranformer_forward
    result = single_block_forward(
  File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/flux/block.py", line 304, in single_block_forward
    attn_output = attn_forward(
  File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/flux/block.py", line 29, in attn_forward
    value = attn.to_v(hidden_states)
  File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/site-packages/peft/tuners/lora/layer.py", line 727, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 27.69 MiB is free. Process 54955 has 79.12 GiB memory in use. Of the allocated memory 77.39 GiB is allocated by PyTorch, and 919.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank0]:     return _run_code(code, main_globals, None,
[rank0]:   File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/runpy.py", line 86, in _run_code
[rank0]:     exec(code, run_globals)
[rank0]:   File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py", line 39, in <module>
[rank0]:     cli.main()
[rank0]:   File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 430, in main
[rank0]:     run()
[rank0]:   File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 284, in run_file
[rank0]:     runpy.run_path(target, run_name="__main__")
[rank0]:   File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 321, in run_path
[rank0]:     return _run_module_code(code, init_globals, run_name,
[rank0]:   File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 135, in _run_module_code
[rank0]:     _run_code(code, mod_globals, init_globals,
[rank0]:   File "/home/admin/code-server-data/extensions/ms-python.python-2023.14.0/pythonFiles/lib/python/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 124, in _run_code
[rank0]:     exec(code, run_globals)
[rank0]:   File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 1342, in <module>
[rank0]:     main(args)
[rank0]:   File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 987, in main
[rank0]:     loss, grad_norm = train_one_step(
[rank0]:   File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 755, in train_one_step
[rank0]:     new_log_probs = grpo_one_step(
[rank0]:   File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/train_grpo_flux.py", line 352, in grpo_one_step
[rank0]:     pred = tranformer_forward(
[rank0]:   File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/flux/transformer.py", line 209, in tranformer_forward
[rank0]:     result = single_block_forward(
[rank0]:   File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/flux/block.py", line 304, in single_block_forward
[rank0]:     attn_output = attn_forward(
[rank0]:   File "/mnt/workspace/user/zengdawei.zdw/shared/jinghan.zq/DiffusionRL/src/flux/block.py", line 29, in attn_forward
[rank0]:     value = attn.to_v(hidden_states)
[rank0]:   File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/jinghan.zq/.conda/envs/aigc_rl/lib/python3.10/site-packages/peft/tuners/lora/layer.py", line 727, in forward
[rank0]:     result = result + lora_B(lora_A(dropout(x))) * scaling
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 27.69 MiB is free. Process 54955 has 79.12 GiB memory in use. Of the allocated memory 77.39 GiB is allocated by PyTorch, and 919.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
